# Raspe um Di√°rio Oficial e contribua com o Querido Di√°rio üï∑Ô∏èüìö

O [Querido Di√°rio](https://queridodiario.ok.org.br/) √© um projeto de c√≥digo aberto da [Open Knowledge Brasil](https://ok.org.br/) que utiliza Python e outras tecnologias para libertar informa√ß√µes do Di√°rio Oficial (DO) das administra√ß√µes p√∫blicas no Brasil. A iniciativa mapeia, baixa e converte todas as p√°ginas das publica√ß√µes para um formato mais acess√≠vel, a fim de facilitar a an√°lise de dados.

Neste tutorial, mostraremos orienta√ß√µes gerais para construir um raspador e contribuir com o projeto Querido Di√°rio.

Se voc√™ prefere uma apresenta√ß√£o sobre o projeto em v√≠deo, confira o workshop [Querido Di√°rio: hoje eu tornei um Di√°rio Oficial acess√≠vel](https://escoladedados.org/coda2020/workshop-querido-diario/) da Ana Paula Gomes no [Coda.Br 2020](https://escoladedados.org/coda2020). Ainda que mudan√ßas recentes possam ter alterado detalhes apresentados na oficina, o v√≠deo √© uma √≥tima complementa√ß√£o a este tutorial. Voc√™ pode utilizar a *timestamp* na descri√ß√£o do v√≠deo para assistir apenas trechos de seu interesse.

## Sum√°rio üìë
  1. [Colabore com o tutorial](#colabore-com-o-tutorial-)
  2. [Mapeando e escolhendo Di√°rios Oficiais](#mapeando-e-escolhendo-diarios-oficiais-)
  3. [Construindo o raspador](#construindo-o-raspador-)
  4. [Configurando um ambiente de desenvolvimento](#configurando-um-ambiente-de-desenvolvimento-)
  5. [Conhecendo os raspadores](#conhecendo-os-raspadores-)
      1. [Casos particulares](#casos-particulares)
  6. [Anatomia de um raspador](#anatomia-de-um-raspador-)
      1. [Par√¢metros iniciais](#parametros-iniciais)
      2. [Par√¢metros de sa√≠da](#parametros-de-saida)
  7. [Hello world: fa√ßa sua primeira requisi√ß√£o](#hello-world-faca-sua-primeira-requisicao-)
  8. [Dissecando o log](#dissecando-o-log-)
  9. [Construindo um raspador de verdade](#construindo-um-raspador-de-verdade-)
      1. [Identificando e testando os seletores](#identificando-e-testando-os-seletores)
      2. [Construindo o c√≥digo do raspador](#construindo-o-codigo-do-raspador)
      3. [Dicas para testar o raspador](#dcas-para-testar-o-raspador)
  10. [Enviando sua contribui√ß√£o](#enviando-sua-contribuicao-)
  11. [Tarefas pendentes](#tarefas-pendentes-)

## Colabore com o tutorial üí™

Este documento est√° em constante constru√ß√£o. Voc√™ pode ajudar a melhorar esta documenta√ß√£o fazendo *pull requests* neste reposit√≥rio.

## Mapeando e escolhendo Di√°rios Oficiais üîé

Existem formas de colaborar com o Querido Di√°rio sem precisar programar. Voc√™ pode participar do [Censo](https://censo.ok.org.br/), por exemplo, e ajudar a mapear os Di√°rios Oficiais de todos os munic√≠pios brasileiros.

Se voc√™ quiser botar a m√£o na massa e construir seu raspador, pode come√ßar ‚Äúadotando‚Äù uma cidade. Primeiro, encontre uma cidade que ainda n√£o esteja listada no [arquivo CITIES.md do reposit√≥rio](https://github.com/okfn-brasil/querido-diario/blob/main/CITIES.md).

O endere√ßo do reposit√≥rio do projeto √©: https://github.com/okfn-brasil/querido-diario/

Antes de come√ßar a trabalhar, vale tamb√©m dar uma olhada na se√ß√£o [Issues](https://github.com/okfn-brasil/querido-diario/issues) e [Pull Requests](https://github.com/okfn-brasil/querido-diario/pulls). Assim, voc√™ consegue checar se j√° existe um raspador para a cidade escolhida que ainda n√£o tenha sido incorporado ao projeto (*Pull Requests*) ou se h√° outras pessoas trabalhando no c√≥digo para o munic√≠pio (*Issues*).

Se o raspador da sua cidade n√£o consta como feito no [arquivo CITIES.md do reposit√≥rio](https://github.com/okfn-brasil/querido-diario/blob/main/CITIES.md), n√£o est√° na se√ß√£o [Issues](https://github.com/okfn-brasil/querido-diario/issues), nem na aba de [Pull requests](https://github.com/okfn-brasil/querido-diario/pulls), ent√£o, crie uma *Issue* nova para anunciar que voc√™ ir√° trabalhar no raspador da cidade escolhida.

## Construindo o raspador üíª

Para acompanhar o tutorial e construir um raspador, √© necess√°rio instalar e conhecer algo sobre os seguintes softwares:

- Uso do terminal
- [Python](https://www.python.org/) e o pacote Scrapy
- [Git](https://git-scm.com/) e Github
- HTML,CSS,XPath

Se voc√™ n√£o se sente confort√°vel com estas tecnologias, sugerimos a leitura dos seguintes tutoriais primeiro.

- [Tutorial da documenta√ß√£o do Scrapy](https://docs.scrapy.org/en/latest/intro/tutorial.html)

- [Introdu√ß√£o a XPath para raspagem de dados](https://escoladedados.org/tutoriais/xpath-para-raspagem-de-dados-em-html/)

- [Git Handbook](https://guides.github.com/introduction/git-handbook/)

## Configurando um ambiente de desenvolvimento üå±

[Fa√ßa um fork do reposit√≥rio](https://docs.github.com/pt/github/getting-started-with-github/quickstart/fork-a-repo) do Querido Di√°rio na sua conta no Github.

Em seguida, clone este novo reposit√≥rio para seu computador e crie uma nova branch para a cidade que ir√° trabalhar:

```sh
git checkout -b <sigladoestado>-<cidade>
```

Vejamos um exemplo com a cidade Paul√≠nia em S√£o Paulo:

```sh
git checkout -b sp-paulinia
```

Se voc√™ usa Windows, baixe as [Ferramentas de Build do Visual Studio](https://visualstudio.microsoft.com/pt-br/downloads/#build-tools-for-visual-studio-2019) e execute o instalador. Durante a instala√ß√£o, selecione a op√ß√£o ‚ÄúDesenvolvimento para desktop com C++‚Äù e finalize o processo.

Se voc√™ usa Linux ou Mac Os, pode simplesmente executar os seguintes comandos. Eles tamb√©m est√£o descritos no README do projeto, na parte de configura√ß√£o de ambiente.

```py
python3 -m venv .venv
source .venv/bin/activate
pip install -r data_collection/requirements.txt
pre-commit install
```

Usu√°rios de Windows devem executar os mesmo comandos, apenas trocando o `source .venv/bin/activate` por `.venv\Scripts\activate.bat`.

## Conhecendo os raspadores üï∑

Todos os raspadores do projeto ficam na pasta [data_collection/gazette/spiders/](https://github.com/okfn-brasil/querido-diario/tree/main/data_collection/gazette/spiders). Navegue por diferentes arquivos e repare no que h√° de comum e diferente no c√≥digo de cada um.

Os nomes de todos os arquivos seguem o padr√£o: **uf_nome_da_cidade.py**.

Ou seja, primeiro, temos a sigla da UF, seguido de _underline_ e nome da cidade. Tudo em min√∫sculas, sem espa√ßos, acentos ou caracteres especiais e separando as palavras com _underline_.

Para se familiarizar, sugerimos que voc√™ navegue por alguns exemplos paradigm√°ticos de Di√°rios Oficiais:

* **Pagina√ß√£o**: um bom exemplo de raspador onde as publica√ß√µes est√£o separadas em v√°rias p√°ginas √© o [da cidade de Manaus](https://github.com/okfn-brasil/querido-diario/blob/main/data_collection/gazette/spiders/am_manaus.py).

* **Busca de datas**: outra situa√ß√£o comum √© quando voc√™ precisa preencher um formul√°rio e fazer uma busca de datas para acessar as publica√ß√µes. √â caso por exemplo do script [ba_salvador.py](https://github.com/okfn-brasil/querido-diario/blob/main/data_collection/gazette/spiders/ba_salvador.py), que raspa as informa√ß√µes da capital baiana.

* **Consulta via APIs**: pode ser tamb√©m que ao analisar as requisi√ß√µes do site, voc√™ descubra uma API escondida, com dados dos documentos j√° organizadas em um arquivo JSON, por exemplo. √â o caso do raspador de [Natal](https://github.com/okfn-brasil/querido-diario/blob/main/data_collection/gazette/spiders/rn_natal.py). Na Escola de Dados, √© poss√≠vel encontrar um webinar sobre [raspagem de dados por meio de "APIs escondidas"](https://escoladedados.org/2021/05/como-descobrir-apis-escondidas-para-facilitar-a-raspagem-de-dados/), que pode ser √∫til para quem est√° come√ßando.

### Casos particulares

Voc√™ talvez tenha reparado que alguns raspadores praticamente n√£o possuem c√≥digo e quase se repetem entre si. Neste caso, tratam-se de munic√≠pios que compartilham o mesmo sistema de publica√ß√£o. Ent√£o, tratamos eles conjuntamente, modificando apenas o necess√°rio de raspador para raspador, ao inv√©s de repetir o mesmo c√≥digo em cada arquivo. √â o caso, por exemplo, de cidades em Santa Catarina como [**Abdon Batista**](https://github.com/okfn-brasil/querido-diario/blob/main/data_collection/gazette/spiders/sc_abdon_batista.py) e [**Agrol√¢ndia**](https://github.com/okfn-brasil/querido-diario/blob/main/data_collection/gazette/spiders/sc_agrolandia.py).

Existem raspadores que n√£o t√™m nome de cidade pois diversos munic√≠pios usam a mesma plataforma para publicar seus Di√°rios Oficiais. S√£o normalmente sites de associa√ß√µes de munic√≠pios. √â o caso de [**ba_associacao_municipios.py**](https://github.com/okfn-brasil/querido-diario/blob/main/data_collection/gazette/spiders/ba_associacao_municipios.py).

Mas para uma primeira contribui√ß√£o n√£o se preocupe com esses casos particulares. Vamos voltar ao nosso exemplo e ver como construir um raspador completo para apenas uma cidade.

## Anatomia de um raspador üß†

Por padr√£o, todos os raspadores come√ßam importando alguns pacotes. Vejamos quais s√£o:

* `import datetime`: pacote para lidar com datas.
* `from gazette.items import Gazette`: item que ser√° salvo com campos de metadados que devem/podem ser preenchidos.
* `from gazette.spiders.base import BaseGazetteSpider`: √© o raspador (spider) base do projeto, que j√° traz algumas funcionalidades √∫teis.

### Par√¢metros iniciais

Cada raspador traz uma classe em Python, que executa determinadas rotinas para cada p√°gina dos sites que publicam Di√°rios Oficiais. Todas as classes possuem pelo menos as informa√ß√µes b√°sicas abaixo:

* `name`: Nome do raspador no mesmo padr√£o do nome do arquivo, sem a extens√£o. Exemplo: `sp_paulinia`.
* `TERRITORY_ID`: c√≥digo da cidade no IBGE. Confira a o arquivo [`territories.csv`](https://github.com/okfn-brasil/querido-diario/blob/main/data_collection/gazette/resources/territories.csv) do projeto para descobrir o c√≥digo da sua cidade. Exemplo: `2905206`.
* `allowed_domains`: Dom√≠nios nos quais o raspador ir√° atuar. Tenha aten√ß√£o aos colchetes. Eles indicam que se trata de uma lista, ainda que tenha apenas um elemento. Exemplo: `["paulinia.sp.gov.br"]`
* `start_urls`: lista com URLs de in√≠cio da navega√ß√£o do raspador (normalmente apenas uma). A resposta dessa requisi√ß√£o inicial √© encaminhada para a vari√°vel `response`, do m√©todo padr√£o do Scrapy chamado `parse`. Veremos mais sobre isso em breve. Novamente, aten√ß√£o aos colchetes. Exemplo:`["http://www.paulinia.sp.gov.br/semanarios/"]`
* `start_date`: Representa√ß√£o de data no formato ano, m√™s e dia (YYYY, M, D) com `datetime.date`. √â a data inicial da publica√ß√£o do Di√°rio Oficial no sistema quest√£o, ou seja, a data da primeira publica√ß√£o dispon√≠vel online. Encontre esta data pesquisando e a insira manualmente nesta vari√°vel. Exemplo: `datetime.date(2017, 4, 3)`.

### Par√¢metros de sa√≠da

Al√©m disso, cada raspador tamb√©m precisa retornar algumas informa√ß√µes por padr√£o. Isso acontece usando a express√£o `yield` nos itens criados do tipo `Gazette`.

* `date`: A data da publica√ß√£o do di√°rio. 
* `file_urls`: Retorna as URLs da publica√ß√£o do DO como uma lista. Um documento pode ter mais de uma URL, mas n√£o √© algo comum.
* `power`: Aceita os par√¢metros `executive` ou `executive_legislative`. Aqui, definimos se o DO tem informa√ß√µes apenas do poder executivo ou tamb√©m do legislativo. Para definir isso, √© preciso olhar manualmente nas publica√ß√µes se h√° informa√ß√µes da C√¢mara Municipal agregadas no mesmo documento, por exemplo.
* `is_extra_edition`: Sinalizamos aqui se √© uma edi√ß√£o extra do Di√°rio Oficial ou n√£o. Edi√ß√µes extras s√£o edi√ß√µes completas do di√°rio que s√£o publicadas fora do calend√°rio previsto.
* `edition_number`: N√∫mero da edi√ß√£o do DO em quest√£o.

Vejamos agora nosso c√≥digo de exemplo.

## Hello world: fa√ßa sua primeira requisi√ß√£o üëã

O Scrapy come√ßa fazendo uma requisi√ß√£o para a URL definida no par√¢metro `start_urls`. A resposta dessa requisi√ß√£o vai para o m√©todo padr√£o `parse`, que ir√° armazenar a resposta na vari√°vel `response`.

Ent√£o, uma forma de fazer um "Hello, world!" no projeto Querido Di√°rio seria com um c√≥digo como este abaixo.

```python
import datetime
from gazette.items import Gazette
from gazette.spiders.base import BaseGazetteSpider

class SpPauliniaSpider(BaseGazetteSpider):
    name = "sp_paulinia"
    TERRITORY_ID = "2905206"
    start_date = datetime.date(2010, 1, 4)
    allowed_domains = ["paulinia.sp.gov.br"]
    start_urls = ["http://www.paulinia.sp.gov.br/semanarios/"]

    def parse(self, response):
        yield Gazette(
            date=datetime.date.today(),
            file_urls=[response.url],
            power="executive",
        )
```

O c√≥digo baixa o HTML da URL inicial, mas n√£o descarrega nenhum DO de fato. Definimos este par√¢metro como o dia de hoje, apenas para ter uma vers√£o b√°sica operacional do c√≥digo. Por√©m, ao construir um raspador real, neste par√¢metro voc√™ dever√° indicar as datas corretas das publica√ß√µes.

De todo modo, isso d√° as bases para voc√™ entender como os raspadores operam e por onde come√ßar a desenvolver o seu pr√≥prio.

Para rodar o c√≥digo, voc√™ pode seguir as seguintes etapas:

1. Crie um arquivo na pasta `data_collection/gazette/spiders/` do reposit√≥rio criado no seu computador a partir do seu fork do Querido Di√°rio;
2. Abra o terminal na ra√≠z do projeto;
3. Ative o ambiente virtual, caso n√£o tenha feito antes, como indicado na se√ß√£o _"[Configurando um ambiente de desenvolvimento](#configurando-um-ambiente-de-desenvolvimento)"_ (`source .venv/bin/activate`, por exemplo);
4. No terminal, v√° para a pasta `data_collection`;
5. No terminal, rode o raspador com o comando `scrapy crawl nome_do_raspador` (nome que est√° no atributo `name` da classe do raspador). Ou seja, no exemplo rodamos: `scrapy crawl sp_paulinia`.

## Dissecando o log üìÑ

Se tudo deu certo, deve aparecer um log enorme terminal.

Ele come√ßa com algo como `[scrapy.utils.log] INFO: Scrapy 2.4.1 started (bot: gazette)` e traz uma s√©rie de informa√ß√µes sobre o ambiente inicialmente. Mas a parte que mais nos interessa come√ßa apenas ap√≥s a linha `[scrapy.core.engine] INFO: Spider opened` e termina na linha `[scrapy.core.engine] INFO: Closing spider (finished)`. Vejamos abaixo.

![](img/output1.png)

A linha `DEBUG: Scraped from <200 http://www.paulinia.sp.gov.br/semanarios/>` nos indica conseguimos acessar o endere√ßo especificado (c√≥digo 200).

Ao desenvolver um raspador, busque principalmente por avisos de *WARNING* e *ERROR*. S√£o eles que trar√£o as informa√ß√µes mais importantes para voc√™ entender os problemas que ocorrem.

Depois de encerrado o raspador, temos as linhas da se√ß√£o dos *monitors*, que trar√° um relat√≥rio de execu√ß√£o. √â normal que apare√ßam erros, como este abaixo.

![Exemplo de erro do Scrapy](img/scrapy_erro.png)
<!-- Imagem gerada no site carbon.now.sh -->

√â um aviso que nada foi raspado nos √∫ltimos dias. Tudo bem, este √© apenas um teste inicial para irmos nos familiarizando com o projeto.

## Construindo um raspador de verdade üõ†Ô∏è

Aqui, tudo vai depender da forma como cada site √© constru√≠do. Mas separamos algumas dicas gerais que podem te ajudar.

Primeiro, navegue pelo site para entender a forma como os DOs est√£o disponibilizados. Busque encontrar um padr√£o consistente e pouco sucet√≠vel a mudan√ßas ocasionais para o rob√¥ extrair as informa√ß√µes necess√°rias. Por exemplo, se as publica√ß√µes est√£o separadas em v√°rias abas ou v√°rias p√°ginas, primeiro certifique-se de que todas elas seguem o mesmo padr√£o. Sendo o caso, ent√£o, voc√™ pode come√ßar fazendo o raspador para a p√°gina mais recente e depois repetir as etapas para as demais, por meio de um loop, por exemplo.

Como vimos, a vari√°vel `response` nos retorna todo conte√∫do da p√°gina inicial do nosso raspador. Ela tem v√°rios atributos, como o `text`, que traz todo HTML da p√°gina em quest√£o como uma *string*. Mas n√£o temos interesse em todo HTML da p√°gina, apenas em informa√ß√µes espec√≠ficas, ent√£o, todo trabalho da raspagem consiste justamente em separar o joio do trigo para filtrar os dados de nosso interesse. Fazemos isso por meio de seletores CSS, XPath ou express√µes regulares.

### Identificando e testando os seletores

Uma forma f√°cil para testar os seletores do seu raspador √© usando o Scrapy Shell. Experimente rodar por exemplo o comando `scrapy shell "http://www.paulinia.sp.gov.br/semanarios"`. Agora, voc√™ pode interagir com a p√°gina por meio da linha de comando e deve ver os comandos que temos dispon√≠veis.

![Output do Scrapy Shell](img/scrapy_shell.png)
<!-- Imagem gerada no site carbon.now.sh -->

O elemento mais importante no nosso caso √© o `response`. Se o acesso ao site foi feito com √™xito, este comando dever√° retornar o c√≥digo 200.

J√° o comando `response.css("a")` nos retornaria informa√ß√µes sobre todos os links das p√°gina em quest√£o. Tamb√©m √© poss√≠vel usar o `response.xpath` para identificar os seletores.

O modo mais f√°cil para de fato identificar os tais seletores que iremos utilizar √© por meio do "Inspetor Web". Trata-se de uma fun√ß√£o dispon√≠vel em praticamente todos navegadores navegadores modernos. Basta clicar do lado direito na p√°gina e selecionar a op√ß√£o "Inspecionar". Assim, podemos visualizar o c√≥digo HTML, copiar e buscar por seletores XPath e CSS.

Experimente rodar o comando `response.xpath("//div[@class='container body-content']//div[@class='row']//a[contains(@href, 'AbreSemanario')]/@href")` e ver os resultados. Este seletor XPath busca primeiro por tags `div` em qualquer lugar da p√°gina, que tenha como classe `container body-content`. Dentro destas tags, buscamos ent√£o por outras `div` com a classe `row`. E, em qualquer lugar dentro destas √∫ltimas, por fim, buscamos por tags `a` (links) cujo atributo `href` contenha a palavra `AbreSemanario` e pedimos para retornar o valor apenas do atributo `href`. 

Existem v√°rias formas de escrever seletores para o mesmo objeto. Voc√™ pode ter uma ideia de como montar o seletor inspecionando a p√°gina que disponibiliza os DOs.

Se voc√™ rodar o comando acima, ir√° ver uma lista de objetos como este: `<Selector xpath="//div[@class='container body-content']//div[@class='row']//a[contains(@href, 'AbreSemanario')]/@href" data='AbreSemanario.aspx?id=1064'>`.

O que realmente nos interessa √© aquilo que est√° dentro do par√¢metro `data`, ou seja, o trecho da URL que nos permite acesso a cada publica√ß√£o. Ent√£o, adicione o `getall()` ao fim do comando anterior: `response.xpath("//div[@class='container body-content']//div[@class='row']//a[contains(@href, 'AbreSemanario')]/@href").getall()`.

Se o objetivo fosse selecionar apenas o primeiro item da lista, poder√≠amos usar o `.get()`.

Por vezes, pode ser necess√°rio utilizar express√µes regulares (regex) para "limpar" os seletores. A [Escola de Dados tem um tutorial sobre o assunto](https://escoladedados.org/tutoriais/expressao-regular-pode-melhorar-sua-vida/) e voc√™ vai encontrar diversos outros materiais na internet com exemplos de regex comuns, como este que aborda [express√µes para identificar datas](https://www.oreilly.com/library/view/regular-expressions-cookbook/9781449327453/ch04s04.html) - algo que pode ser muito √∫til na hora de trabalhar com DOs.

Ap√≥s identificar os seletores, √© hora de construir seu raspador no arquivo `.py` da pasta `spiders`.

### Construindo o c√≥digo do raspador

Normalmente, para completar o seu raspador voc√™ precisar√° fazer algumas requisi√ß√µes extras. √â poss√≠vel identificar quais requisi√ß√µes s√£o necess√°rias fazer atrav√©s do "Analizador de Rede" em navegadores. A [palestra do Giulio Carvalho na Python Brasil 2020](https://youtu.be/nhEPZ3r5zGY) mostra como pode ser feita essa an√°lise de requisi√ß√µes de um site para depois converter em um raspador para o Querido Di√°rio.

Se voc√™ precisar fazer alguma requisi√ß√£o `GET`, o objeto de requisi√ß√£o `scrapy.Request` deve ser o suficiente. O objeto `scrapy.FormRequest` normalmente √© usado para requisi√ß√µes `POST`, que enviam algum dado no `formdata`.

Sempre que uma requisi√ß√£o for feita a partir de uma p√°gina, ela √© feita utilizando a express√£o `yield` e sua resposta ser√° enviada para algum m√©todo da classe do raspador. As requisi√ß√µes tem alguns par√¢metros essenciais (outros par√¢metros podem ser vistos na documenta√ß√£o do Scrapy):

* `url`: A URL da p√°gina que ser√° acessada;
* `callback`: O m√©todo da classe do raspador para o qual a resposta ser√° enviada (por padr√£o, o m√©todo `parse` √© utilizado);
* `formdata` (em `FormRequest`): Um dicion√°rio contendo campos e seus valores que ser√£o enviados.

No exemplo completo para a cidade de Paul√≠nia (SP), na p√°gina inicial temos links para todos os anos onde h√° di√°rios dispon√≠veis e em cada ano todos os di√°rios s√£o listados na mesma p√°gina. Ent√£o, uma requisi√ß√£o √© feita para acessar a p√°gina de cada ano (a p√°gina inicial j√° √© o ano atual) usando `scrapy.FormRequest` (nesse caso, um m√©todo `.from_response` que j√° aproveita muitas coisas da `response` atual, inclusive a pr√≥pria URL). A resposta dessa requisi√ß√£o deve ir para o m√©todo `parse_year` que ir√° extrair todos os metadados poss√≠veis de ser encontrados na p√°gina. Com isso, a extra√ß√£o dos di√°rios de Paul√≠nia est√° completa üòÑ.

Veja como fica o raspador no exemplo a seguir (com coment√°rios para explicar algumas partes do c√≥digo):

```python
# Importa√ß√£o dos pacotes necess√°rios
import datetime
import scrapy
from gazette.items import Gazette
from gazette.spiders.base import BaseGazetteSpider

# Defini√ß√£o da classe do raspador
class SpPauliniaSpider(BaseGazetteSpider):

# Par√¢metros iniciais
    name = "sp_paulinia"
    TERRITORY_ID = "2905206"
    start_date = datetime.date(2010, 1, 4)
    allowed_domains = ["www.paulinia.sp.gov.br"]
    start_urls = ["http://www.paulinia.sp.gov.br/semanarios"]

    # O parse abaixo ir√° partir da start_url acima
    def parse(self, response):
        # Nosso seletor cria uma lista com os c√≥digo HTML onde os anos est√£o localizados
        years = response.css("div.col-md-1")
        # E fazer um loop para extrair de fato o ano
        for year in years:
            # Para cada item da lista (year) vamos pegar (get) um seletor XPath.
            # Tamb√©m dizemos que queremos o resultado como um n√∫mero inteiro (int)
            year_to_scrape = int(year.xpath("./a/font/text()").get())

            # Para n√£o fazer requisi√ß√µes desnecess√°rias, se o ano j√° for o da p√°gina
            # inicial (p√°gina inicial √© o ano atual) ou ent√£o for anterior ao ano da
            # data inicial da busca, n√£o iremos fazer a requisi√ß√£o
            if (
                year_to_scrape < self.start_date.year
                or year_to_scrape == datetime.date.today().year
            ):
                continue

            # Com Scrapy √© poss√≠vel utilizar regex direto no elemento com os m√©todos
            # `.re` e `.re_first` (na maioria das vezes √© suficiente e n√£o precisamos
            # usar m√©todos da biblioteca `re`)
            event_target = year.xpath("./a/@href").re_first(r"(ctl00.*?)',")

            # O m√©todo `.from_response` nesse caso √© bem √∫til pois pega v√°rios
            # elementos do tipo <input> que j√° est√£o dentro do elemento <form>
            # localizado na p√°gina e preenche eles automaticamente no formdata, assim
            # √© poss√≠vel economizar muitas linhas de c√≥digo
            yield scrapy.FormRequest.from_response(
                response,
                formdata={"__EVENTTARGET": event_target},
                callback=self.parse_year,
            )

        # O `yield from` permite fazermos `yield` em cada resultado do m√©todo gerador
        # `self.parse_year`, assim, aqui estamos dando `yield` em todos os itens
        # `Gazette` raspados da p√°gina inicial
        yield from self.parse_year(response)

    def parse_year(self, response):
        editions = response.xpath(
            "//div[@class='container body-content']//div[@class='row']//a[contains(@href, 'AbreSemanario')]"
        )

        for edition in editions:
            document_href = edition.xpath("./@href").get()

            title = edition.xpath("./text()")

            gazette_date = datetime.datetime.strptime(
                title.re_first(r"\d{2}/\d{2}/\d{4}"), "%d/%m/%Y"
            ).date()
            edition_number = title.re_first(r"- (\d+) -")
            is_extra_edition = "extra" in title.get().lower()

            # Esse site "esconde" o link direto do PDF por tr√°s de uma s√©rie de
            # redirecionamentos, por√©m, como nas configura√ß√µes do projeto √© permitido
            # que arquivos baixados sofram redirecionamento, √© poss√≠vel colocar o link
            # "falso" j√° no item `Gazette` e o projeto vai conseguir baixar o documento
            yield Gazette(
                date=gazette_date,
                edition_number=edition_number,
                file_urls=[response.urljoin(document_href)],
                is_extra_edition=is_extra_edition,
                power="executive",
            )
```

No final do processo, teste seu raspador usando:

```
scrapy crawl nome_do_raspador
```

### Dicas para testar o raspador

Para ajudar a debugar eventuais problemas na constru√ß√£o do c√≥digo, voc√™ pode inserir a linha `import pdb; pdb.set_trace()` em qualquer trecho do raspador para inspecionar seu c√≥digo (contexto, vari√°veis, etc.) durante a execu√ß√£o.

√â poss√≠vel testar o raspador sem baixar nenhum arquivo adicionando `-s FILES_STORE=""` ao comando. Isso √© √∫til para testar r√°pido se todas as requisi√ß√µes est√£o funcionando.

Tamb√©m podemos adicionar os itens extra√≠dos para um arquivo com `-o output.csv` (ou outra extens√£o como `.json` ou `.jsonlines`), tornando poss√≠vel analisar de forma bem mais f√°cil se o que est√° sendo raspado est√° correto.

Se o log estiver muito grande, √© poss√≠vel adicion√°-lo a um arquivo de texto adicionando `-s LOG_FILE=logs.txt` ao comando, fazendo com que poss√≠veis erros n√£o passem despercebidos.

Tamb√©m √© muito importante testar se o filtro de data no raspador est√° funcionando. Utilizando o argumento `start_date`, apenas as requisi√ß√µes necess√°rias para extrair documentos a partir da data desejada devem ser feitas. Este teste pode ser feito com `-a start_date=2020-12-01` (para publica√ß√µes a partir de 1 de dezembro de 2020). O atributo `start_date` do raspador √© utilizado internamente, ent√£o se o argumento n√£o for passado, o padr√£o (primeira data de publica√ß√£o) √© utilizado no lugar.

Como exemplo de uso de todas a op√ß√µes anteriores em `sp_paulinia` a partir de 1 de dezembro de 2020:

```
scrapy crawl sp_paulinia -a start_date=2020-12-01 -s FILES_STORE="" -s LOG_FILE=logs.txt -o output.json
```

## Enviando sua contribui√ß√£o ü§ù

Ao fazer o commit do c√≥digo, mencione a issue do raspador da sua cidade. Voc√™ pode incluir uma mensagem como `Close #20`, por exemplo, onde #20 √© o n√∫mero identificador da issue criada. Tamb√©m adicione uma descri√ß√£o comentando suas op√ß√µes na hora de desenvolver o raspador ou eventuais incertezas.

Normalmente adicionar apenas um raspador necessita apenas de um √∫nico commit. Mas, se for necess√°rio mais de um commit, tente manter um certo n√≠vel de separa√ß√£o entre o que cada um est√° fazendo e tamb√©m se certifique que suas mensagens est√£o bem claras e correspondendo ao que os commits realmente fazem.

Uma boa pr√°tica √© sempre atualizar a ramifica√ß√£o (_branch_) que voc√™ est√° desenvolvendo com o que est√° na `main` atualizada do projeto. Assim, se o projeto teve atualiza√ß√µes, voc√™ pode resolver algum conflito antes mesmo de fazer o Pull Request.

Qualquer d√∫vida, abra o seu Pull Request em modo de rascunho (_draft_) e relate suas d√∫vidas para que pessoas do projeto tentem te ajudar üòÉ. O [canal de discuss√µes no Discord](https://discord.com/invite/nDc9p4drm4) tamb√©m √© aberto para tirar d√∫vidas e trocar ideias.

## Tarefas pendentes ‚úîÔ∏è
- [ ] Melhorar a se√ß√£o [Construindo um raspador de verdade](#construindo-um-raspador-de-verdade-)
- [ ] Revisar e incorporar conte√∫dos faltantes (e atuais) citados no artigo do [Vanz](http://jvanz.com/como-funciona-o-robozinho-do-serenata-que-baixa-os-diarios-oficiais.html).
- [ ] Revisar e incorporar conte√∫dos faltantes (e atuais) citados no [post](https://www.anapaulagomes.me/pt-br/2020/10/quero-tornar-di%C3%A1rios-oficiais-acess%C3%ADveis.-como-come%C3%A7ar/) feito pela Ana Paula Gomes.
