# üï∑Ô∏èüìö Raspe um Di√°rio Oficial e contribua com o Querido Di√°rio

O [Querido Di√°rio](https://queridodiario.ok.org.br/) √© um projeto de c√≥digo aberto da [Open Knowledge Brasil](https://ok.org.br/) que utiliza Python e outras tecnologias para libertar informa√ß√µes do Di√°rio Oficial (DO) das administra√ß√µes p√∫blicas no Brasil. A iniciativa mapeia, baixa e converte todas as p√°ginas das publica√ß√µes para um formato mais acess√≠vel, a fim de facilitar a an√°lise de dados.

Neste tutorial, mostraremos algumas orienta√ß√µes gerais para construir um raspador e contribuir com o projeto Querido Di√°rio. 

## üîé Mapeando os Di√°rios Oficiais
Existem formas de colaborar com o Querido Di√°rio sem precisar programar. Voc√™ pode participar de nosso Censo, por exemplo, e ajudar a mapear os Di√°rios Oficiais de todos os munic√≠pios brasileiros.

Se voc√™ quiser botar a m√£o na massa e construir seu raspador, pode come√ßar ‚Äúadotando‚Äù uma cidade. Primeiro, encontre uma cidade que ainda n√£o esteja listado no [arquivo CITIES.md do reposit√≥rio](https://github.com/okfn-brasil/querido-diario/blob/main/CITIES.md). 

O endere√ßo do reposit√≥rio do projeto √©: https://github.com/okfn-brasil/querido-diario/

Para acompanhar o tutorial e construir um raspador, √© necess√°rio algum conhecimento sobre:

- Uso do terminal
- Python e o pacote Scrapy
- Git e Github
- HTML,CSS, XPath


### Pareceu grego?

Se voc√™ n√£o se sente confort√°vel com estas tecnologias, sugerimos a leitura dos seguintes tutoriais primeiro.

- [Tutorial da documenta√ß√£o do Scrapy](https://docs.scrapy.org/en/latest/intro/tutorial.html)

- [Introdu√ß√£o a XPath para raspagem de dados](https://escoladedados.org/tutoriais/xpath-para-raspagem-de-dados-em-html/)


## üå± Configurando um ambiente de desenvolvimento
Fa√ßa um fork do reposit√≥rio oficial do Querido Di√°rio na sua conta no Github.

Em seguida, clone este novo reposit√≥rio para seu computador.

Se voc√™ usa Windows, baixe as [Ferramentas de Build do Visual Studio](https://visualstudio.microsoft.com/pt-br/downloads/#build-tools-for-visual-studio-2019) e execute o instalador. Durante a instala√ß√£o, selecione a op√ß√£o ‚ÄúDesenvolvimento para desktop com C++‚Äù e finalize o processo.

Se voc√™ usa Linux ou Mac Os, pode simplesmente executar os seguintes comandos. Eles tamb√©m est√£o descritos no README do projeto, na parte de configura√ß√£o de ambiente.

```
python3 -m venv .venv
source .venv/bin/activate
pip install -r data_collection/requirements.txt
pre-commit install
```

Usu√°rios de Windows devem executar os mesmo comandos, apenas trocando o segundo deles por:  `.venv\Scripts\activate.bat`


## üï∑ Conhecendo os raspadores

Todos os raspadores do projeto ficam na pasta [data_collection/gazette/spiders/](https://github.com/okfn-brasil/querido-diario/tree/main/data_collection/gazette/spiders). Navegue por diferentes arquivos e repare no que h√° de comum e diferente no c√≥digo de cada um.

Os nomes de todos os arquivos seguem o padr√£o: **uf_nomedacidade.py**. 

Ou seja, primeiro, temos a sigla da UF, seguido de underline e nome da cidade. Tudo em min√∫sculas, sem espa√ßos, acentos ou caracteres especiais.

Veja alguns exemplos paradigm√°ticos de Di√°rios Oficiais:

* **Pagina√ß√£o**: um bom exemplo de raspador de DO onde as publica√ß√µes est√£o separadas em v√°rias p√°ginas √© o [script da cidade de Manaus](https://github.com/okfn-brasil/querido-diario/blob/main/data_collection/gazette/spiders/am_manaus.py).

* **Busca de datas**: outra situa√ß√£o comum √© quando voc√™ precisa preencher um formul√°rio e fazer uma busca de datas para acessar as publica√ß√µes. √â caso por exemplo do script [ba_salvador.py](https://github.com/okfn-brasil/querido-diario/blob/main/data_collection/gazette/spiders/ba_salvador.py), que raspa as informa√ß√µes da capital baiana.

* **Consulta via APIs**: pode ser tamb√©m que os dados sobre as publica√ß√µes estejam dispon√≠veis via API, j√° organizados em um arquivo JSON. √â o caso do raspador de [Natal](https://github.com/okfn-brasil/querido-diario/blob/main/data_collection/gazette/spiders/rn_natal.py).

Se voc√™ navegou pelos raspadores, talvez tenha reparado que alguns c√≥digos possuem apenas os metadados. Neste caso, tratam-se de munic√≠pios que compartilham o mesmo sistema de publica√ß√£o. Ent√£o, tratamos eles conjuntamente, como associa√ß√µes de munic√≠pios, ao inv√©s de repetir o mesmo raspador em cada arquivo.

Mas n√£o se preocupe com isso, por ora. Vamos voltar ao nosso exemplo e ver como construir um raspador completo individualmente.

## üß† Anatomia de um raspador

![image](https://user-images.githubusercontent.com/3240562/130146622-5c5a406a-14c3-4867-a292-3e00fd5961b4.png)

<!-- Imagem gerada no site carbon.now.sh -->

Por padr√£o, todos os raspadores come√ßam importando alguns pacotes. Vejamos quais s√£o.

`import datetime`: pacote para lidar com datas.

`import scrapy`: quem faz quase toda m√°gica acontecer. √â o pacote utilizado para construir nossos raspadores.

`from gazette.spiders.base import BaseGazetteSpider`: √© o raspador (spider) base do projeto, que j√° traz v√°rias funcionalidades √∫teis.

### Par√¢metros inicias

Cada raspador traz uma classe em Python, que executa determinadas rotinas para cada URL de Di√°rios Oficiais. Todas as classes possuem pelo menos as informa√ß√µes b√°sicas abaixo.

Vejamos um exemplo a partir da cidade Paul√≠nia em S√£o Paulo.

`name` = Nome do raspador no mesmo padr√£o do nome do arquivo, sem a extens√£o. Exemplo: `sp_paulinia`.

`TERRITORY_ID` = c√≥digo da cidade no IBGE. Confira esta tabela da Wikipedia para descobrir o c√≥digo da sua cidade. Exemplo: `2905206`.

`allowed_domains` = Dom√≠nios nos quais o raspador ir√° atuar. Exemplo: `["www.paulinia.sp.gov.br/"]`

`start_urls` = URL de in√≠cio da navega√ß√£o do raspador. A resposta dessa requisi√ß√£o inicial √© encaminhada para a vari√°vel response, do m√©todo padr√£o do Scrapy chamado parse. Veremos mais sobre isso em breve. Exemplo:`["http://www.paulinia.sp.gov.br/semanarios/"]`

`start_date` = Representa√ß√£o de data no formato ano, m√™s e dia (YYYY, M, D), usando o pacote datetime. √â a data inicial da publica√ß√£o do Di√°rio Oficial no sistema quest√£o, ou seja, a data da primeira publica√ß√£o dispon√≠vel online. Encontre esta data pesquisando e inserindo essa data manualmente nesta vari√°vel. Exemplo: `datetime.date(2017, 4, 3)`.

### Par√¢metros de sa√≠da

Al√©m disso, cada raspador tamb√©m precisa retornar algumas informa√ß√µes por padr√£o. Isso acontece usando a fun√ß√£o `yield`.

`date` = A data da publica√ß√£o em quest√£o.

`file_urls` = Retorna a URL das publica√ß√µes do DO. 

`power` = Aceita os par√¢metros `executive` ou `executive_legislative`. Aqui, definimos se o DO tem informa√ß√µes apenas do poder executivo ou tamb√©m do legislativo. Para definir isso, √© preciso olhar manualmente nas publica√ß√µes se h√° informa√ß√µes da C√¢mara Municipal agregadas no mesmo documento, por exemplo.

`is_extra_edition` = Sinalizamos aqui se √© uma edi√ß√£o extra do Di√°rio Oficial ou n√£o.

`edition_number` = N√∫mero da edi√ß√£o do DO em quest√£o.

# üëã Hello world: fa√ßa sua primeira requisi√ß√£o

O Scrapy come√ßa fazendo uma requisi√ß√£o para a URL definida no par√¢metro `start_urls`. A resposta dessa requisi√ß√£o vai para o m√©todo padr√£o `parse`, que ir√° armazenar a resposta na vari√°vel `response`.

A vari√°vel `response` tem v√°rios atributos, como o `text`, que traz o HTML da p√°gina em quest√£o como uma *string*.

Ent√£o, voc√™ pode uma forma de fazer um famoso "Hello, world!" no projeto Querido Di√°rio seria com um c√≥digo mais ou menos como este abaixo. Voc√™ encontra o c√≥digo abaixo no arquivo [sp_paulinia.py](sp_paulina.py), presente neste reposit√≥rio.

Para testar um raspador e come√ßar a desenvolver o seu, siga as seguintes etapas:

1. Importe o arquivo para a pasta `data_collection/gazette/spiders/` no reposit√≥rio criado no seu computador.
2. Abra o terminal nesta pasta.
3. Ative o ambiente virtual, caso n√£o tenha feito antes. Rode `source .venv/bin/activate` ou o comando adequado na pasta onde o ambiente foi criado.
4. No terminal, rode o raspador com o comando `scrapy crawl nomedoraspador`. Ou seja, no exemplo rodamos: `scrapy crawl sp_paulinia`.

# üí• Dissecando o arquivo log

**Bum!!** Deve aparecer um arquivo de log enorme terminal. 

A parte que nos interessa come√ßa apenas ap√≥s a linha **[scrapy.core.engine] INFO: Spider opened**.

como ler o log?
ver a partir do INFO: Spider opened
buscar principalmente por WARNING e ERROR


# üõ†Ô∏è Construindo um raspador

Aqui, tudo vai depender da forma como cada site √© constru√≠do. Mas separamos algumas dicas gerais que podem te ajudar.

Primeiro, identifique um seletor que retorne todas as publica√ß√µes separadamente. Se as publica√ß√µes est√£o separadas em v√°rias abas ou v√°rias p√°ginas, primeiro certifique-se de que todas elas seguem o mesmo padr√£o. Sendo o caso, ent√£o, voc√™ pode come√ßar fazendo o raspador para a p√°gina mais recente e depois repetir as etapas para as demais, por meio de um loop, por exemplo.

Para testar os seletores e construir o raspador, voc√™ pode utilizar algumas destas alternativas:

Inspetor Web

scrapy shell
scrapy shell "http://www.paulinia.sp.gov.br/semanarios"


https://www.anapaulagomes.me/pt-br/2020/10/quero-tornar-di%C3%A1rios-oficiais-acess%C3%ADveis.-como-come%C3%A7ar/


